"""
M√≥dulo de implementa√ß√£o do agente de IA
"""
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage

from ..config import settings
from ..tools import get_all_tools
from ..prompts import prompt_loader


class ConversationalAgent:
    """Agente conversacional com mem√≥ria e ferramentas"""
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ):
        """
        Inicializa o agente conversacional
        
        Args:
            model_name: Nome do modelo OpenAI (padr√£o: configurado em .env)
            temperature: Temperatura do modelo (padr√£o: configurado em .env)
            max_tokens: M√°ximo de tokens na resposta (padr√£o: configurado em .env)
        """
        # Valida as configura√ß√µes
        settings.validate()
        
        # Configura par√¢metros do modelo
        self.model_name = model_name or settings.OPENAI_MODEL
        self.temperature = temperature if temperature is not None else settings.TEMPERATURE
        self.max_tokens = max_tokens or settings.MAX_TOKENS
        
        # Inicializa o modelo LLM
        self.llm = ChatOpenAI(
            model=self.model_name,
            temperature=self.temperature,
            max_tokens=self.max_tokens,
            openai_api_key=settings.OPENAI_API_KEY,
        )
        
        # Carrega ferramentas
        self.tools = get_all_tools()
        
        # Vincula as ferramentas ao modelo
        self.llm_with_tools = self.llm.bind_tools(self.tools)
        
        # Configura o prompt do sistema com data atual
        base_system_prompt = prompt_loader.get_system_prompt()
        hoje = datetime.now()
        data_atual = hoje.strftime("%d/%m/%Y")
        data_atual_yyyymmdd = hoje.strftime("%Y%m%d")
        data_30_dias = (hoje + timedelta(days=30)).strftime("%Y%m%d")
        
        self.system_prompt = f"{base_system_prompt}\n\nüìÖ CONTEXTO TEMPORAL IMPORTANTE:\n" \
                             f"Data atual: {data_atual} (formato API: {data_atual_yyyymmdd})\n" \
                             f"Ao consultar editais no PNCP, a data final (dataFinal) DEVE ser maior ou igual √† data atual.\n" \
                             f"Para consultas futuras, calcule a data no formato YYYYMMDD a partir de hoje.\n" \
                             f"Exemplos: 'pr√≥ximo m√™s' use uma data 30-60 dias √† frente, " \
                             f"'este m√™s' use o final do m√™s atual, 'daqui 30 dias' = {data_30_dias}"
        
        # Inicializa a mem√≥ria da conversa
        self.chat_history = []
    
    def _execute_tool(self, tool_name: str, tool_input: Any) -> str:
        """
        Executa uma ferramenta
        
        Args:
            tool_name: Nome da ferramenta
            tool_input: Entrada da ferramenta
        
        Returns:
            Resultado da ferramenta
        """
        print(f"\nüîß EXECUTANDO FERRAMENTA: {tool_name}")
        print(f"üì• Par√¢metros: {tool_input}")
        
        for tool in self.tools:
            if tool.name == tool_name:
                try:
                    # Usar invoke() ao inv√©s de func() para que o LangChain
                    # desempacote corretamente os par√¢metros do dicion√°rio
                    result = tool.invoke(tool_input)
                    # Mostra preview do resultado (primeiras 200 caracteres)
                    preview = result[:200] + "..." if len(result) > 200 else result
                    print(f"üì§ Resultado (preview): {preview}")
                    return result
                except Exception as e:
                    error_msg = f"Erro ao executar ferramenta: {str(e)}"
                    print(f"‚ùå {error_msg}")
                    print(f"üîç Debug - tool_input type: {type(tool_input)}")
                    print(f"üîç Debug - tool_input value: {tool_input}")
                    return error_msg
        
        error_msg = f"Ferramenta '{tool_name}' n√£o encontrada."
        print(f"‚ùå {error_msg}")
        return error_msg
    
    def chat(self, user_input: str, max_iterations: int = 15) -> str:
        """
        Envia uma mensagem para o agente e retorna a resposta
        
        Args:
            user_input: Mensagem do usu√°rio
            max_iterations: N√∫mero m√°ximo de itera√ß√µes (padr√£o: 15 para consultas complexas)
        
        Returns:
            Resposta do agente
        """
        print("\n" + "="*100)
        print("ü§ñ AGENTE CONTRATAI - INICIANDO PROCESSAMENTO")
        print("="*100)
        print(f"üí¨ Pergunta do usu√°rio: {user_input}")
        print(f"‚öôÔ∏è  Max itera√ß√µes: {max_iterations}")
        print("="*100)
        
        try:
            # Constr√≥i a lista de mensagens com o hist√≥rico
            messages = [SystemMessage(content=self.system_prompt)]
            messages.extend(self.chat_history)
            messages.append(HumanMessage(content=user_input))
            
            # Loop de execu√ß√£o do agente
            for i in range(max_iterations):
                print(f"\nüîÑ ITERA√á√ÉO {i+1}/{max_iterations}")
                print("-" * 100)
                
                # Invoca o modelo com as ferramentas
                print(f"üß† Invocando modelo {self.model_name}...")
                response = self.llm_with_tools.invoke(messages)
                
                # Verifica se h√° tool calls
                if not response.tool_calls:
                    # N√£o h√° mais ferramentas a chamar, retorna a resposta
                    print("\n‚úÖ RESPOSTA FINAL GERADA (sem tool calls)")
                    output = response.content
                    print(f"üí≠ Resposta: {output[:150]}{'...' if len(output) > 150 else ''}")
                    
                    # Adiciona √† mem√≥ria
                    self.chat_history.append(HumanMessage(content=user_input))
                    self.chat_history.append(AIMessage(content=output))
                    
                    # Limita o tamanho da mem√≥ria (mant√©m √∫ltimas 20 mensagens)
                    if len(self.chat_history) > 20:
                        self.chat_history = self.chat_history[-20:]
                    
                    print("\n" + "="*100)
                    print("‚ú® PROCESSAMENTO CONCLU√çDO COM SUCESSO")
                    print("="*100 + "\n")
                    return output
                
                # Adiciona a resposta do modelo √†s mensagens
                print(f"\nüõ†Ô∏è  Modelo solicitou {len(response.tool_calls)} chamada(s) de ferramenta")
                messages.append(response)
                
                # Executa as ferramentas chamadas
                for idx, tool_call in enumerate(response.tool_calls, 1):
                    print(f"\nüìå Tool Call {idx}/{len(response.tool_calls)}")
                    tool_name = tool_call["name"]
                    tool_input = tool_call["args"]
                    tool_call_id = tool_call["id"]
                    
                    # Executa a ferramenta
                    tool_output = self._execute_tool(tool_name, tool_input)
                    
                    # Adiciona o resultado da ferramenta √†s mensagens como ToolMessage
                    messages.append(
                        ToolMessage(
                            content=tool_output,
                            tool_call_id=tool_call_id,
                        )
                    )
            
            # Se chegou aqui, atingiu o n√∫mero m√°ximo de itera√ß√µes
            print("\n" + "="*100)
            print(f"‚ö†Ô∏è  LIMITE DE ITERA√á√ïES ATINGIDO ({max_iterations})")
            print("="*100 + "\n")
            erro_msg = "Desculpe, n√£o consegui completar a tarefa. A consulta pode ser muito complexa ou " \
                      "pode haver um problema tempor√°rio. Tente: (1) reformular a pergunta de forma mais simples, " \
                      "(2) dividir em perguntas menores, ou (3) tentar novamente em alguns instantes."
            return erro_msg
        
        except Exception as e:
            error_msg = f"Erro ao processar mensagem: {str(e)}"
            print("\n" + "="*100)
            print(f"‚ùå ERRO NO PROCESSAMENTO")
            print("="*100)
            print(f"üí• {error_msg}")
            print("="*100 + "\n")
            
            # Retorna mensagem de erro amig√°vel
            agent_prompts = prompt_loader.get_agent_prompts()
            return agent_prompts.get("error_message", "Desculpe, ocorreu um erro.")
    
    def clear_history(self):
        """Limpa o hist√≥rico da conversa"""
        self.chat_history = []
    
    def get_history(self) -> List[Dict[str, str]]:
        """
        Retorna o hist√≥rico da conversa em formato de lista de dicion√°rios
        
        Returns:
            Lista com o hist√≥rico formatado
        """
        history = []
        for msg in self.chat_history:
            if isinstance(msg, HumanMessage):
                history.append({"role": "user", "content": msg.content})
            elif isinstance(msg, AIMessage):
                history.append({"role": "assistant", "content": msg.content})
        return history


def create_agent(
    model_name: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
) -> ConversationalAgent:
    """
    Fun√ß√£o auxiliar para criar uma inst√¢ncia do agente
    
    Args:
        model_name: Nome do modelo OpenAI
        temperature: Temperatura do modelo
        max_tokens: M√°ximo de tokens na resposta
    
    Returns:
        Inst√¢ncia do ConversationalAgent
    """
    return ConversationalAgent(
        model_name=model_name,
        temperature=temperature,
        max_tokens=max_tokens,
    )
